[2024-07-19T20:58:26.085+0000] {local_task_job_runner.py:120} INFO - ::group::Pre task execution logs
[2024-07-19T20:58:26.106+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-19T20:58:25.504369+00:00 [queued]>
[2024-07-19T20:58:26.112+0000] {taskinstance.py:2076} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-19T20:58:25.504369+00:00 [queued]>
[2024-07-19T20:58:26.112+0000] {taskinstance.py:2306} INFO - Starting attempt 1 of 1
[2024-07-19T20:58:26.122+0000] {taskinstance.py:2330} INFO - Executing <Task(PythonOperator): reddit_extraction> on 2024-07-19 20:58:25.504369+00:00
[2024-07-19T20:58:26.127+0000] {standard_task_runner.py:64} INFO - Started process 69 to run task
[2024-07-19T20:58:26.129+0000] {standard_task_runner.py:90} INFO - Running: ['airflow', 'tasks', 'run', 'etl_reddit_pipeline', 'reddit_extraction', 'manual__2024-07-19T20:58:25.504369+00:00', '--job-id', '12', '--raw', '--subdir', 'DAGS_FOLDER/reddit_dag.py', '--cfg-path', '/tmp/tmp8czk45ue']
[2024-07-19T20:58:26.130+0000] {standard_task_runner.py:91} INFO - Job 12: Subtask reddit_extraction
[2024-07-19T20:58:26.167+0000] {task_command.py:426} INFO - Running <TaskInstance: etl_reddit_pipeline.reddit_extraction manual__2024-07-19T20:58:25.504369+00:00 [running]> on host aa0946d1cd0d
[2024-07-19T20:58:26.235+0000] {taskinstance.py:2648} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='nopega' AIRFLOW_CTX_DAG_ID='etl_reddit_pipeline' AIRFLOW_CTX_TASK_ID='reddit_extraction' AIRFLOW_CTX_EXECUTION_DATE='2024-07-19T20:58:25.504369+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2024-07-19T20:58:25.504369+00:00'
[2024-07-19T20:58:26.235+0000] {taskinstance.py:430} INFO - ::endgroup::
[2024-07-19T20:58:26.248+0000] {logging_mixin.py:188} INFO - connect to reddit!
[2024-07-19T20:58:26.249+0000] {logging_mixin.py:188} INFO - <praw.models.listing.generator.ListingGenerator object at 0x7f0ed8194880>
[2024-07-19T20:58:27.589+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "tl;dr: Can you be a data enginner without coding skills and just use no or low-code tools like Alteryx to do the job?\n\nI've been in analytics and data visualization for well over 10 years. The tools I use every day are Alteryx and Tableau. I'm our department's Alteryx server admin as well as mentor. I help train newbies on Alteryx and Tableau as well. One of the things I enjoy the most about the job is the ETL piece from Alteryx. Just like any part of analytics the hardest part of it is data wrangling piece; which I enjoy quite a bit. BUT, I cannot code to save my life. I can do basic SQL. I had learned SQL right before I learned Alteryx many years ago, so I haven't had to learn advanced SQL becuse Alteryx can do it all in the GUI. I failed C++ twice in college(I'm 44) and have attempted to teach myself Python 3 times in the past 4 years and can't really understand it to do anything sufficient enough to be considered usable for a job. This helps explain why i use Alteryx and Tableau. The other viz tools like Qlik(blaaaahhhhh) and Looker are much more code-heavy.", 'author_fullname': 't2_4tu9x', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Can you be a data engineer without knowing advanced coding?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6qny1', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.83, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 72, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 72, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721349036.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>tl;dr: Can you be a data enginner without coding skills and just use no or low-code tools like Alteryx to do the job?</p>\n\n<p>I&#39;ve been in analytics and data visualization for well over 10 years. The tools I use every day are Alteryx and Tableau. I&#39;m our department&#39;s Alteryx server admin as well as mentor. I help train newbies on Alteryx and Tableau as well. One of the things I enjoy the most about the job is the ETL piece from Alteryx. Just like any part of analytics the hardest part of it is data wrangling piece; which I enjoy quite a bit. BUT, I cannot code to save my life. I can do basic SQL. I had learned SQL right before I learned Alteryx many years ago, so I haven&#39;t had to learn advanced SQL becuse Alteryx can do it all in the GUI. I failed C++ twice in college(I&#39;m 44) and have attempted to teach myself Python 3 times in the past 4 years and can&#39;t really understand it to do anything sufficient enough to be considered usable for a job. This helps explain why i use Alteryx and Tableau. The other viz tools like Qlik(blaaaahhhhh) and Looker are much more code-heavy.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e6qny1', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='csh8428'), 'discussion_type': None, 'num_comments': 78, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6qny1/can_you_be_a_data_engineer_without_knowing/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e6qny1/can_you_be_a_data_engineer_without_knowing/', 'subreddit_subscribers': 198067, 'created_utc': 1721349036.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.589+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hello, Hope everyone is doing good.\n\nI’m currently working as a Data Engineer ( almost a senior data engineer now ) in a Big 4 consultancy firm for past 2 years. My tech stack is heavily revolved around Microsoft Azure ( Data Factory, Synapse warehouse, Fabric, Data Lakes, SQL databases, Log Analytics ), Databricks, dbt, Azure DevOps ( CICD and git ), and a little bit of PowerBI. \n\nOver the years, I feel like that I am the jack of all trades aka when there is work I get it done, but I lack in depth understanding of the technology ( master of none ). For example, I know SQL but never used any window, pivot functions up till now so I am not really hands on with it. I’ll be able to build anything from scratch but debugging an error on an existing solution makes me go insane.\n\nI feel like my scope to data engineering tech stack is very limited as well.\n\nAny advices for me on how do I approach this problem and be the master of the underlying tech, and how do I develop more broad business acumen? Please feel free to drop in your suggestions and additional advices you have for me.\n\nThank you :)', 'author_fullname': 't2_8k7ziok4', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Jack of all, master of none', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6px9k', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.91, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 55, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 55, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1721350232.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721346889.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hello, Hope everyone is doing good.</p>\n\n<p>I’m currently working as a Data Engineer ( almost a senior data engineer now ) in a Big 4 consultancy firm for past 2 years. My tech stack is heavily revolved around Microsoft Azure ( Data Factory, Synapse warehouse, Fabric, Data Lakes, SQL databases, Log Analytics ), Databricks, dbt, Azure DevOps ( CICD and git ), and a little bit of PowerBI. </p>\n\n<p>Over the years, I feel like that I am the jack of all trades aka when there is work I get it done, but I lack in depth understanding of the technology ( master of none ). For example, I know SQL but never used any window, pivot functions up till now so I am not really hands on with it. I’ll be able to build anything from scratch but debugging an error on an existing solution makes me go insane.</p>\n\n<p>I feel like my scope to data engineering tech stack is very limited as well.</p>\n\n<p>Any advices for me on how do I approach this problem and be the master of the underlying tech, and how do I develop more broad business acumen? Please feel free to drop in your suggestions and additional advices you have for me.</p>\n\n<p>Thank you :)</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e6px9k', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='KeyboaRdWaRRioR1214'), 'discussion_type': None, 'num_comments': 61, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6px9k/jack_of_all_master_of_none/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e6px9k/jack_of_all_master_of_none/', 'subreddit_subscribers': 198067, 'created_utc': 1721346889.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.590+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I\'ve worked in data engineering my whole career.  My current role is essentially "software engineer - data", but I have also worked in roles that were more adhoc SQL queries and building data pipelines.\n\n\n\n\n\nLet\'s say you are currently working in your tech stack of choice, what would it take you to switch to a higher paying role in data engineering that has a less interesting tech stack?', 'author_fullname': 't2_auf0obxj', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How important is tech stack vs. total compensation to you in data engineering?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e71c1f', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': 'transparent', 'subreddit_type': 'public', 'ups': 49, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': '02917a1a-ac9d-11eb-beee-0ed0a94b470d', 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 49, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721388143.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;ve worked in data engineering my whole career.  My current role is essentially &quot;software engineer - data&quot;, but I have also worked in roles that were more adhoc SQL queries and building data pipelines.</p>\n\n<p>Let&#39;s say you are currently working in your tech stack of choice, what would it take you to switch to a higher paying role in data engineering that has a less interesting tech stack?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': 'Senior Data Engineer', 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e71c1f', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='level_126_programmer'), 'discussion_type': None, 'num_comments': 40, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': 'dark', 'permalink': '/r/dataengineering/comments/1e71c1f/how_important_is_tech_stack_vs_total_compensation/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e71c1f/how_important_is_tech_stack_vs_total_compensation/', 'subreddit_subscribers': 198067, 'created_utc': 1721388143.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.590+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi everyone,\n\nI'm currently tasked with transferring a large number of small JSON files between S3 buckets. The files are currently organized by year, month, day, and time (e.g., YYYYMMDDHHMMSS). I need to reorganize them so that they are stored only by year, month, and day (e.g., YYYYMMDD).\n\nI'm considering using AWS Glue for this task. Here are a few approaches I'm thinking about:\n\n1. **Direct Copy:** Simply copying the files from one location to another using s3.copyobject. Is this the most straightforward approach using Glue notebooks?\n2. **Using PySpark:** Leveraging PySpark in Glue to handle the file transfer. Would this be more efficient for handling a large number of files?\n3. **Merge/Compression Strategy:** Merging/Zipping the files daily before transferring, then unzipping them in the new bucket. Would this reduce costs or improve transfer efficiency?\n\nI'm looking for advice on the most effective way to approach this task using AWS Glue. Any insights on the best practices for such file manipulations or experiences with similar data engineering challenges would be greatly appreciated!", 'author_fullname': 't2_c3x6xj5p', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Best way to move 10,000 JSON files from one S3 bucket to another?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e765oq', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.96, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 21, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 21, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1721402412.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721402058.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>I&#39;m currently tasked with transferring a large number of small JSON files between S3 buckets. The files are currently organized by year, month, day, and time (e.g., YYYYMMDDHHMMSS). I need to reorganize them so that they are stored only by year, month, and day (e.g., YYYYMMDD).</p>\n\n<p>I&#39;m considering using AWS Glue for this task. Here are a few approaches I&#39;m thinking about:</p>\n\n<ol>\n<li><strong>Direct Copy:</strong> Simply copying the files from one location to another using s3.copyobject. Is this the most straightforward approach using Glue notebooks?</li>\n<li><strong>Using PySpark:</strong> Leveraging PySpark in Glue to handle the file transfer. Would this be more efficient for handling a large number of files?</li>\n<li><strong>Merge/Compression Strategy:</strong> Merging/Zipping the files daily before transferring, then unzipping them in the new bucket. Would this reduce costs or improve transfer efficiency?</li>\n</ol>\n\n<p>I&#39;m looking for advice on the most effective way to approach this task using AWS Glue. Any insights on the best practices for such file manipulations or experiences with similar data engineering challenges would be greatly appreciated!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e765oq', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='cryptoyash'), 'discussion_type': None, 'num_comments': 34, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e765oq/best_way_to_move_10000_json_files_from_one_s3/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e765oq/best_way_to_move_10000_json_files_from_one_s3/', 'subreddit_subscribers': 198067, 'created_utc': 1721402058.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.591+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "There's a common issue across many organisations where user/customer entered data has to be cleaned, validated and processed into a warehouse. The target table in the warehouse is of course well defined, but the 'schema' of the source is a mess. The source files are often human readable - think ProductCode, product\\_code. \\[prod code\\] as a tiny example, but is there something that can automatically map the columns as required. \n\nTo make things interesting the columns will move around, in that some customers will send the data with columns in different order. Is there a tool or achitecture that can handle that kind of vairable input? I wrote one a while back in C# and it worked pretty well, but I don't want to have to go down that route again. \n\nBTW The data itself will come as csv, xlsx, txt, maybe even .accdb. \n\nAny products to take away that pain?", 'author_fullname': 't2_6sbag', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'How do you handle messy data from customers?', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6xy1q', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.87, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721374089.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>There&#39;s a common issue across many organisations where user/customer entered data has to be cleaned, validated and processed into a warehouse. The target table in the warehouse is of course well defined, but the &#39;schema&#39; of the source is a mess. The source files are often human readable - think ProductCode, product_code. [prod code] as a tiny example, but is there something that can automatically map the columns as required. </p>\n\n<p>To make things interesting the columns will move around, in that some customers will send the data with columns in different order. Is there a tool or achitecture that can handle that kind of vairable input? I wrote one a while back in C# and it worked pretty well, but I don&#39;t want to have to go down that route again. </p>\n\n<p>BTW The data itself will come as csv, xlsx, txt, maybe even .accdb. </p>\n\n<p>Any products to take away that pain?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e6xy1q', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='MrTelly'), 'discussion_type': None, 'num_comments': 28, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6xy1q/how_do_you_handle_messy_data_from_customers/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e6xy1q/how_do_you_handle_messy_data_from_customers/', 'subreddit_subscribers': 198067, 'created_utc': 1721374089.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.591+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I recently started using ADF(Azure Data Factory) for creating pipelines. I am using Tumbling Window trigger to trigger my pipeline and using a custom logic in it using the start and end time of the window for Incremental Data Load( only processing the newly added files since the last pipeline run). Is it the optimal way to do Incremental Data Load in ADF and if not then is there any other way people do it in general?\n\nWhat techniques do u guys use for the same?', 'author_fullname': 't2_nur6n6ds', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Incremental Data Load in ADF', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6wf4h', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.93, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 11, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 11, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721368068.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I recently started using ADF(Azure Data Factory) for creating pipelines. I am using Tumbling Window trigger to trigger my pipeline and using a custom logic in it using the start and end time of the window for Incremental Data Load( only processing the newly added files since the last pipeline run). Is it the optimal way to do Incremental Data Load in ADF and if not then is there any other way people do it in general?</p>\n\n<p>What techniques do u guys use for the same?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e6wf4h', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Effective-Tie-3149'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6wf4h/incremental_data_load_in_adf/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e6wf4h/incremental_data_load_in_adf/', 'subreddit_subscribers': 198067, 'created_utc': 1721368068.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.591+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I just want to make something somewhat close to what a data engineer would be doing. I know the scale and complexity of a personal beginner project and an actual project are worlds apart, but I would like to NOT make noobie mistakes/practices that might make me look bad. \n\nHere’s what I’m working on for more context: Pull top x trending videos info daily from Youtube API and store in ***. Eventually want to transform and send the data to a dashboard.\n\nUsing python for scripting, dbt for data transformation. Will use airflow to execute this every day. Everything is inside docker containers. Any suggestions?\n', 'author_fullname': 't2_7l5s31ok', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Turn offs and Turn ons in personal projects? ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6t9sy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.85, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 9, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 9, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721357075.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I just want to make something somewhat close to what a data engineer would be doing. I know the scale and complexity of a personal beginner project and an actual project are worlds apart, but I would like to NOT make noobie mistakes/practices that might make me look bad. </p>\n\n<p>Here’s what I’m working on for more context: Pull top x trending videos info daily from Youtube API and store in ***. Eventually want to transform and send the data to a dashboard.</p>\n\n<p>Using python for scripting, dbt for data transformation. Will use airflow to execute this every day. Everything is inside docker containers. Any suggestions?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e6t9sy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='HarvesterOfReveries'), 'discussion_type': None, 'num_comments': 5, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6t9sy/turn_offs_and_turn_ons_in_personal_projects/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e6t9sy/turn_offs_and_turn_ons_in_personal_projects/', 'subreddit_subscribers': 198067, 'created_utc': 1721357075.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.592+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi All, \n\nI'm a data scientist/data analyst at a mid-sized corporation (< 10k employees). Our company uses snowflake as it's main repository for structured data. I work on a small analytics team, one of several at the company. \n\nCurrently, we have no way to develop code in a development environment and ship to a separate production environment. We only have a production database, and no CI/CD tool to speak off. \n\nThat worked fine until recently when I realized that I was developing and shipping code in the same environment, which was causing issues. I realized that we needed a CI/CD tool if we wanted to do things correctly and avoid shipping code with bugs in it. \n\nSo I went off and attempted to design a custom CI/CD solution for our team. I thought it would be relatively simple to wrangle something up using python scripts and Github actions, but the complexity quickly caught up to me, and I'm realizing I likely bit off more than I can chew. I'm not a data engineer, and CI/CD was a completely foreign concept to me up until about 3 weeks ago (as my work until recently only involved creating views on top of data, not modifying data tables themselves).\n\nI'm not sure what other teams do for their CI/CD processes, if they are doing anything. I've put out a question to them, and I'll see what they say. \n\nI think the right solution is to use a tool like dbt, however dbt cloud is prohibitively expensive (at $100/user/month, with 8 users on our team, we're already over budget). \n\nI was wondering if anyone had any suggestions for solutions? We're looking for a tool that can help us isolate a production and development environment, track changes to each environment, and version control, at the minimum. \n\n  \nThanks - \n\n  \n", 'author_fullname': 't2_81rlkfyr', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'DBT or Homebrew custom CI/CD Solution', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6pfo4', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 7, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 7, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721345518.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi All, </p>\n\n<p>I&#39;m a data scientist/data analyst at a mid-sized corporation (&lt; 10k employees). Our company uses snowflake as it&#39;s main repository for structured data. I work on a small analytics team, one of several at the company. </p>\n\n<p>Currently, we have no way to develop code in a development environment and ship to a separate production environment. We only have a production database, and no CI/CD tool to speak off. </p>\n\n<p>That worked fine until recently when I realized that I was developing and shipping code in the same environment, which was causing issues. I realized that we needed a CI/CD tool if we wanted to do things correctly and avoid shipping code with bugs in it. </p>\n\n<p>So I went off and attempted to design a custom CI/CD solution for our team. I thought it would be relatively simple to wrangle something up using python scripts and Github actions, but the complexity quickly caught up to me, and I&#39;m realizing I likely bit off more than I can chew. I&#39;m not a data engineer, and CI/CD was a completely foreign concept to me up until about 3 weeks ago (as my work until recently only involved creating views on top of data, not modifying data tables themselves).</p>\n\n<p>I&#39;m not sure what other teams do for their CI/CD processes, if they are doing anything. I&#39;ve put out a question to them, and I&#39;ll see what they say. </p>\n\n<p>I think the right solution is to use a tool like dbt, however dbt cloud is prohibitively expensive (at $100/user/month, with 8 users on our team, we&#39;re already over budget). </p>\n\n<p>I was wondering if anyone had any suggestions for solutions? We&#39;re looking for a tool that can help us isolate a production and development environment, track changes to each environment, and version control, at the minimum. </p>\n\n<p>Thanks - </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e6pfo4', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nazstat'), 'discussion_type': None, 'num_comments': 17, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6pfo4/dbt_or_homebrew_custom_cicd_solution/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e6pfo4/dbt_or_homebrew_custom_cicd_solution/', 'subreddit_subscribers': 198067, 'created_utc': 1721345518.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.593+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey everyone, it's been a while since I did any kimball data modelling. Let's say I have two fact tables, one at daily granularity holding charges and payments against a customer. The other is a monthly balances holding the opening balance, total charges, total payments, and closing balance against the customer at month granularity. What's the best way to handle joining the different granularity fact tables on to a date dimension table? \n\n(1) Hold a start of month date field or something like that to allow allow a join condition for the monthly fact data that will not duplicate the records for every day in the month?\n\n(2) Create two date dimension tables (one at daily granularity and one at monthly granularity). \n\nI'm trying to consider the pros and cons of each approach. Or if I have missed something obvious please let me know. Thanks. ", 'author_fullname': 't2_qwv7rzfhd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Kimball modelling question', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e76e9o', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721402676.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey everyone, it&#39;s been a while since I did any kimball data modelling. Let&#39;s say I have two fact tables, one at daily granularity holding charges and payments against a customer. The other is a monthly balances holding the opening balance, total charges, total payments, and closing balance against the customer at month granularity. What&#39;s the best way to handle joining the different granularity fact tables on to a date dimension table? </p>\n\n<p>(1) Hold a start of month date field or something like that to allow allow a join condition for the monthly fact data that will not duplicate the records for every day in the month?</p>\n\n<p>(2) Create two date dimension tables (one at daily granularity and one at monthly granularity). </p>\n\n<p>I&#39;m trying to consider the pros and cons of each approach. Or if I have missed something obvious please let me know. Thanks. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e76e9o', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Altruistic_Menu_8312'), 'discussion_type': None, 'num_comments': 9, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e76e9o/kimball_modelling_question/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e76e9o/kimball_modelling_question/', 'subreddit_subscribers': 198067, 'created_utc': 1721402676.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.593+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hey all,\n\nI'm a data engineer with several years of experience. However, most of my experience has been in ETL pipeline development/maintenance, data cleaning and validation, testing, and database management/query tuning. I've held roles where data modeling decisions were already made and I had to learn to work with those decisions.\n\n  \nI'd like to learn more about data modeling. While I intend on reading up on the most commonly recommended books, I tend to learn most efficiently by following real world examples. \n\n  \nI'm wondering if there are any resources you know of that have such examples? I'm not talking about a 'pseudo code' version of data modeling i.e. here are our dim tables and fact tables, here are some columns, and that's it. But more about which structure was selected, why certain decisions were made, how the structure benefits specific teams and stakeholders etc. Does something like that exist?  \n", 'author_fullname': 't2_gcq3x', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'detailed real-world data modeling examples', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e72uc6', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721393068.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hey all,</p>\n\n<p>I&#39;m a data engineer with several years of experience. However, most of my experience has been in ETL pipeline development/maintenance, data cleaning and validation, testing, and database management/query tuning. I&#39;ve held roles where data modeling decisions were already made and I had to learn to work with those decisions.</p>\n\n<p>I&#39;d like to learn more about data modeling. While I intend on reading up on the most commonly recommended books, I tend to learn most efficiently by following real world examples. </p>\n\n<p>I&#39;m wondering if there are any resources you know of that have such examples? I&#39;m not talking about a &#39;pseudo code&#39; version of data modeling i.e. here are our dim tables and fact tables, here are some columns, and that&#39;s it. But more about which structure was selected, why certain decisions were made, how the structure benefits specific teams and stakeholders etc. Does something like that exist?  </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e72uc6', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='johnsonfrusciante'), 'discussion_type': None, 'num_comments': 2, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e72uc6/detailed_realworld_data_modeling_examples/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e72uc6/detailed_realworld_data_modeling_examples/', 'subreddit_subscribers': 198067, 'created_utc': 1721393068.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.593+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_rsfc3f7tc', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '7 Ways to Make Queries Faster', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 80, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e74k3j', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.73, 'author_flair_background_color': None, 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/IUn6zzYbZhIO6zX-tJmsOYlQa_n-8HlcyvSAPtDohpY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1721397903.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'glaredb.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://glaredb.com/blog/7-ways-to-make-queries-faster', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/1BDGSnjzDgWGMv8eon7FxTKca3ygFhKn6ih_Wjw-7I0.jpg?auto=webp&s=d5939d01647e9ce4edb1c3dc92774d5eb155ff56', 'width': 1792, 'height': 1024}, 'resolutions': [{'url': 'https://external-preview.redd.it/1BDGSnjzDgWGMv8eon7FxTKca3ygFhKn6ih_Wjw-7I0.jpg?width=108&crop=smart&auto=webp&s=75cf94d5a276c882eaabb55734a22a7fd0aa9cb8', 'width': 108, 'height': 61}, {'url': 'https://external-preview.redd.it/1BDGSnjzDgWGMv8eon7FxTKca3ygFhKn6ih_Wjw-7I0.jpg?width=216&crop=smart&auto=webp&s=c958cd5cb0877bcbfadcfb5ad60dcf586a3bebef', 'width': 216, 'height': 123}, {'url': 'https://external-preview.redd.it/1BDGSnjzDgWGMv8eon7FxTKca3ygFhKn6ih_Wjw-7I0.jpg?width=320&crop=smart&auto=webp&s=d3fda807e3df743c8800e053767d7e0538b74f18', 'width': 320, 'height': 182}, {'url': 'https://external-preview.redd.it/1BDGSnjzDgWGMv8eon7FxTKca3ygFhKn6ih_Wjw-7I0.jpg?width=640&crop=smart&auto=webp&s=d6d1268c139effbdf5ee0c6d35a87d9c060de212', 'width': 640, 'height': 365}, {'url': 'https://external-preview.redd.it/1BDGSnjzDgWGMv8eon7FxTKca3ygFhKn6ih_Wjw-7I0.jpg?width=960&crop=smart&auto=webp&s=16ac6012bffb12a972d8b88197ac39351251d0b0', 'width': 960, 'height': 548}, {'url': 'https://external-preview.redd.it/1BDGSnjzDgWGMv8eon7FxTKca3ygFhKn6ih_Wjw-7I0.jpg?width=1080&crop=smart&auto=webp&s=a2aeb5987253f7828967a5fdeac0ee12847a3eb1', 'width': 1080, 'height': 617}], 'variants': {}, 'id': 'N78_fPyFJF8rlVjqyf-wDNdUJfg-OBdMVw4x2fBBfZE'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e74k3j', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='glaredb_tal'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e74k3j/7_ways_to_make_queries_faster/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://glaredb.com/blog/7-ways-to-make-queries-faster', 'subreddit_subscribers': 198067, 'created_utc': 1721397903.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.594+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_5ede2', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': '[GCP] Terraform module to stream Pub/Sub JSON to Cloud SQL', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 78, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e73o09', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/qDEH49m_mA6xLV_E-qLSobZHtOdvq_1rdoByI8p18dY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1721395466.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'dataroc.ca', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.dataroc.ca/blog/terraform-module-to-stream-pubsub-json-to-cloud-sql-on-gcp', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/iz8MbcoGH-mZTCW1c1RYnQ3r5s8yNvwr8tc2P6AfPSw.jpg?auto=webp&s=d43c12c0a648194f2de7cb8cfc3d44190426c0fb', 'width': 1920, 'height': 1080}, 'resolutions': [{'url': 'https://external-preview.redd.it/iz8MbcoGH-mZTCW1c1RYnQ3r5s8yNvwr8tc2P6AfPSw.jpg?width=108&crop=smart&auto=webp&s=09373d133291d9f0e9bf033b251d4ddd015c0b6e', 'width': 108, 'height': 60}, {'url': 'https://external-preview.redd.it/iz8MbcoGH-mZTCW1c1RYnQ3r5s8yNvwr8tc2P6AfPSw.jpg?width=216&crop=smart&auto=webp&s=806d530832f42a2ad8f51273f6b9b5c0fab02cc0', 'width': 216, 'height': 121}, {'url': 'https://external-preview.redd.it/iz8MbcoGH-mZTCW1c1RYnQ3r5s8yNvwr8tc2P6AfPSw.jpg?width=320&crop=smart&auto=webp&s=2dd522c17b8f87f9bafcef964432b172000f6e20', 'width': 320, 'height': 180}, {'url': 'https://external-preview.redd.it/iz8MbcoGH-mZTCW1c1RYnQ3r5s8yNvwr8tc2P6AfPSw.jpg?width=640&crop=smart&auto=webp&s=50a2e4760b7d2abd3341347314cea949003bb3bc', 'width': 640, 'height': 360}, {'url': 'https://external-preview.redd.it/iz8MbcoGH-mZTCW1c1RYnQ3r5s8yNvwr8tc2P6AfPSw.jpg?width=960&crop=smart&auto=webp&s=903115b7cb9528cec7059f34b88e7738a83f22d7', 'width': 960, 'height': 540}, {'url': 'https://external-preview.redd.it/iz8MbcoGH-mZTCW1c1RYnQ3r5s8yNvwr8tc2P6AfPSw.jpg?width=1080&crop=smart&auto=webp&s=f436c08f8eb10005810dcd082c864f7a5289cce3', 'width': 1080, 'height': 607}], 'variants': {}, 'id': 'kYCxHAa8pHzkS242PMvm-jPwv0-b8P-1mCOHTX9h2u4'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e73o09', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='wil19558'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e73o09/gcp_terraform_module_to_stream_pubsub_json_to/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.dataroc.ca/blog/terraform-module-to-stream-pubsub-json-to-cloud-sql-on-gcp', 'subreddit_subscribers': 198067, 'created_utc': 1721395466.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.594+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi All , after going through multiple posts on reddit on how to integrate DBT as part of your existing data pipeline , there are multiple techniques , however the one where DBT is being used using Astronomer Cosmos over Bigquery as a data platform is being less talked about. As part of an ongoing project , I encountered the same issue and was able to solve the same and documented the journey in order to help in case anyone case across the same set of challenges.\n\nPlease have a read and feel free to encourage the same by clapping and comments to make this post known to a more wider group.\n', 'author_fullname': 't2_1derwnyo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Running DBT with Airflow', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 105, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e71wi8', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'ups': 6, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 6, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/MkGockf8J8lC8BD4d0NkdZeniHKPNRius9vSVIUWNKY.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1721390132.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'towardsdev.com', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi All , after going through multiple posts on reddit on how to integrate DBT as part of your existing data pipeline , there are multiple techniques , however the one where DBT is being used using Astronomer Cosmos over Bigquery as a data platform is being less talked about. As part of an ongoing project , I encountered the same issue and was able to solve the same and documented the journey in order to help in case anyone case across the same set of challenges.</p>\n\n<p>Please have a read and feel free to encourage the same by clapping and comments to make this post known to a more wider group.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://towardsdev.com/from-idea-to-implementation-a-step-by-step-guide-to-starting-your-data-science-side-2701183b536e', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/I7qW4Rj0K9hutI0u1_jVoD0Py55oayktqNp2MmVu7nI.jpg?auto=webp&s=67cc6d9f39da3a3ccc63310278d78eda7acfcfb4', 'width': 1200, 'height': 900}, 'resolutions': [{'url': 'https://external-preview.redd.it/I7qW4Rj0K9hutI0u1_jVoD0Py55oayktqNp2MmVu7nI.jpg?width=108&crop=smart&auto=webp&s=1b9e5f17f00b3d7cb06f84ca7328450c57486492', 'width': 108, 'height': 81}, {'url': 'https://external-preview.redd.it/I7qW4Rj0K9hutI0u1_jVoD0Py55oayktqNp2MmVu7nI.jpg?width=216&crop=smart&auto=webp&s=e6257b24bc9929f5db32dd894069024caff0dca4', 'width': 216, 'height': 162}, {'url': 'https://external-preview.redd.it/I7qW4Rj0K9hutI0u1_jVoD0Py55oayktqNp2MmVu7nI.jpg?width=320&crop=smart&auto=webp&s=782948a06a56baae6c55df73f5cea3d5e13e2c8c', 'width': 320, 'height': 240}, {'url': 'https://external-preview.redd.it/I7qW4Rj0K9hutI0u1_jVoD0Py55oayktqNp2MmVu7nI.jpg?width=640&crop=smart&auto=webp&s=cbc9d3fabc5ff67f7dbd14aa4aee8685ab78981e', 'width': 640, 'height': 480}, {'url': 'https://external-preview.redd.it/I7qW4Rj0K9hutI0u1_jVoD0Py55oayktqNp2MmVu7nI.jpg?width=960&crop=smart&auto=webp&s=c19b65e9202d4b6ce8c46962189a0262b801fcbd', 'width': 960, 'height': 720}, {'url': 'https://external-preview.redd.it/I7qW4Rj0K9hutI0u1_jVoD0Py55oayktqNp2MmVu7nI.jpg?width=1080&crop=smart&auto=webp&s=d04ec6ea2f894f44b6d350f23575f25b3d79c63f', 'width': 1080, 'height': 810}], 'variants': {}, 'id': 'A7RC-VNG1i0eTl8EtU4QfBOSmRbYMY7XtqDIch6_CJI'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e71wi8', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='ps_kev_96'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e71wi8/running_dbt_with_airflow/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://towardsdev.com/from-idea-to-implementation-a-step-by-step-guide-to-starting-your-data-science-side-2701183b536e', 'subreddit_subscribers': 198067, 'created_utc': 1721390132.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.595+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_4oyhx', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'The Data-Related “Red Flags” All Organisations Should Be Aware Of (And How To Avoid Them)', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 73, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6s3jy', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.86, 'author_flair_background_color': None, 'ups': 5, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 5, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/y1Yr3E1KCTecdY3KwzylAEeBkmNN3muOxphMehjHo1o.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1721353409.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'smbtech.au', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://smbtech.au/thought-leadership/the-data-related-red-flags-all-organisations-should-be-aware-of-and-how-to-avoid-them/', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/_Ub0kZRw1HOgR8hl5o1RvHGg0JleXiK4yEZ1n2vD9Bg.jpg?auto=webp&s=529a9cd3b8d7d9b974e115e7bc9b7c9b39d180b6', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/_Ub0kZRw1HOgR8hl5o1RvHGg0JleXiK4yEZ1n2vD9Bg.jpg?width=108&crop=smart&auto=webp&s=9396140babdd3eaac053db6d39aca05d119a6783', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/_Ub0kZRw1HOgR8hl5o1RvHGg0JleXiK4yEZ1n2vD9Bg.jpg?width=216&crop=smart&auto=webp&s=ed695fa4a5a358d7b6203d9d0c61cc90d3895125', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/_Ub0kZRw1HOgR8hl5o1RvHGg0JleXiK4yEZ1n2vD9Bg.jpg?width=320&crop=smart&auto=webp&s=f97f6940629d0855831b41b1ce545f6f8b796757', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/_Ub0kZRw1HOgR8hl5o1RvHGg0JleXiK4yEZ1n2vD9Bg.jpg?width=640&crop=smart&auto=webp&s=3391c9c3a144738f0931ebcc7e9a287b04634a15', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/_Ub0kZRw1HOgR8hl5o1RvHGg0JleXiK4yEZ1n2vD9Bg.jpg?width=960&crop=smart&auto=webp&s=21df1096e961d1196ff42c4e8a1d2a3dd4c424cd', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/_Ub0kZRw1HOgR8hl5o1RvHGg0JleXiK4yEZ1n2vD9Bg.jpg?width=1080&crop=smart&auto=webp&s=ecd90e8b2bfa7a6cc3fd54d58f0c75a3722ffc11', 'width': 1080, 'height': 567}], 'variants': {}, 'id': '3laub5_SLqybTnfwGQgFex-JEkiYvp86_Kbbp_LlPLM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e6s3jy', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='teheditor'), 'discussion_type': None, 'num_comments': 3, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6s3jy/the_datarelated_red_flags_all_organisations/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://smbtech.au/thought-leadership/the-data-related-red-flags-all-organisations-should-be-aware-of-and-how-to-avoid-them/', 'subreddit_subscribers': 198067, 'created_utc': 1721353409.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.596+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "A very intentional feature of DuckDB is [how it handles concurrency](https://duckdb.org/docs/connect/concurrency.html), which only allows one read/write connection, OR multiple read connections at the same time.  So with DuckDB, one cannot have a read/write and a read connection open at the same time.\n\nPersonal preference, but when working with hosted Datawarehouses (redshift/***, etc), I keep the database open in a viewer while tweaking and running data models with DBT, or loading new data. This helps me iterate and develop fast, without having to keep closing the connection.\n\nThat said, I love DuckDB, and I've been working on a flow to do some local data modeling.  Super simple:\n\n* Python to ingest raw data (api, etc) and store as parquet files. \n* In a DuckDB database, create views to query the raw parquet files into tables.\n* Use DBT to model the data to produce additional blended views\n\nOne advantage of this setup is that since the “raw” DuckDB tables are views querying the Parquet files, they update automatically as the Parquet files are updated. This fits nicely into my workflow, allowing me to view the DuckDB database while data is being updated.\n\nHowever modeling data with DBT requires editing the schema, which cannot be done while viewing the database.\n\nTo work around this, I’ve been maintaining separate read and write copies of the DuckDB database file. I keep the read file open in the viewer while making schema changes on the write file, then overwrite the read file and reload in the viewer. While this works, I suspect there are drawbacks.\n\nI’d love to hear the community’s thoughts on this approach. Is there a more sensible way to achieve this with the tools I’m using? I know I can spin up Postgres database w/ Docker, but I’m keen on adapting my workflow to use DuckDB if I can.\n\nTLDR: I’m exploring DuckDB for local data modeling but its concurrency limits clash with my workflow.  I’m seeking advice on better solutions. Any suggestions?", 'author_fullname': 't2_epmxu99uo', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Optimizing my Workflow with DuckDB: Seeking Advice', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e79ybs', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'self', 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721411632.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>A very intentional feature of DuckDB is <a href="https://duckdb.org/docs/connect/concurrency.html">how it handles concurrency</a>, which only allows one read/write connection, OR multiple read connections at the same time.  So with DuckDB, one cannot have a read/write and a read connection open at the same time.</p>\n\n<p>Personal preference, but when working with hosted Datawarehouses (redshift/***, etc), I keep the database open in a viewer while tweaking and running data models with DBT, or loading new data. This helps me iterate and develop fast, without having to keep closing the connection.</p>\n\n<p>That said, I love DuckDB, and I&#39;ve been working on a flow to do some local data modeling.  Super simple:</p>\n\n<ul>\n<li>Python to ingest raw data (api, etc) and store as parquet files. </li>\n<li>In a DuckDB database, create views to query the raw parquet files into tables.</li>\n<li>Use DBT to model the data to produce additional blended views</li>\n</ul>\n\n<p>One advantage of this setup is that since the “raw” DuckDB tables are views querying the Parquet files, they update automatically as the Parquet files are updated. This fits nicely into my workflow, allowing me to view the DuckDB database while data is being updated.</p>\n\n<p>However modeling data with DBT requires editing the schema, which cannot be done while viewing the database.</p>\n\n<p>To work around this, I’ve been maintaining separate read and write copies of the DuckDB database file. I keep the read file open in the viewer while making schema changes on the write file, then overwrite the read file and reload in the viewer. While this works, I suspect there are drawbacks.</p>\n\n<p>I’d love to hear the community’s thoughts on this approach. Is there a more sensible way to achieve this with the tools I’m using? I know I can spin up Postgres database w/ Docker, but I’m keen on adapting my workflow to use DuckDB if I can.</p>\n\n<p>TLDR: I’m exploring DuckDB for local data modeling but its concurrency limits clash with my workflow.  I’m seeking advice on better solutions. Any suggestions?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?auto=webp&s=a3ce4d9713e9b21d12f203bb1557511dedc29060', 'width': 1200, 'height': 630}, 'resolutions': [{'url': 'https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=108&crop=smart&auto=webp&s=b86b56641acbba7143cf836e76a6ed127d0cdc7c', 'width': 108, 'height': 56}, {'url': 'https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=216&crop=smart&auto=webp&s=9f7911437d91a92bdb5eaa0846b1481429176330', 'width': 216, 'height': 113}, {'url': 'https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=320&crop=smart&auto=webp&s=18a3e761649359ef6f006465b214153f3ee22b64', 'width': 320, 'height': 168}, {'url': 'https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=640&crop=smart&auto=webp&s=7d6d7436d826b7e957b7a49ae82dbd0475e0b149', 'width': 640, 'height': 336}, {'url': 'https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=960&crop=smart&auto=webp&s=a85eb7536b7a9cb175fd04a6e566a36738718f7f', 'width': 960, 'height': 504}, {'url': 'https://external-preview.redd.it/JwfNU-pPYQvsMTxfB2iJ5Wh6atTaZ5PuvOLNe3j85AQ.jpg?width=1080&crop=smart&auto=webp&s=73e36fb4e5a8b625ec054bc33b25149637525dd2', 'width': 1080, 'height': 567}], 'variants': {}, 'id': 'jWyiaF4Jb7ULQyU8SCl75THeEbJM9dbSQ9YXdauXufk'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e79ybs', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='blue-lighty'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e79ybs/optimizing_my_workflow_with_duckdb_seeking_advice/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e79ybs/optimizing_my_workflow_with_duckdb_seeking_advice/', 'subreddit_subscribers': 198067, 'created_utc': 1721411632.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.596+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi everyone, \n\nAt my workplace we use MariaDB as a primary database. Currently there were existing stored procedure that do the transformation when data gets into the database. \n\nHowever, I have concerns about the version controlling of the stored procedure. Therefore, the solution I came up with was to store these procedure on Github, and use AWS lambda to execute these procedure. \n\nMay I know is this a good idea? Are they any reasons or scenarios where Stored Procedure is preferred over using python to execute SQL statements?', 'author_fullname': 't2_fcjnd', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Stored Procedure vs Script in Github', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6x721', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.81, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721371087.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone, </p>\n\n<p>At my workplace we use MariaDB as a primary database. Currently there were existing stored procedure that do the transformation when data gets into the database. </p>\n\n<p>However, I have concerns about the version controlling of the stored procedure. Therefore, the solution I came up with was to store these procedure on Github, and use AWS lambda to execute these procedure. </p>\n\n<p>May I know is this a good idea? Are they any reasons or scenarios where Stored Procedure is preferred over using python to execute SQL statements?</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e6x721', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='yiternity'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6x721/stored_procedure_vs_script_in_github/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e6x721/stored_procedure_vs_script_in_github/', 'subreddit_subscribers': 198067, 'created_utc': 1721371087.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.596+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_3297c', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Query Snowflake Iceberg tables with DuckDB & Spark to Save Costs', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 83, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6r5no', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/sDaKDexXp6uYdreBbXSJeqqef8qyejIyVOu_QeIiAvw.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1721350535.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'buremba.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://buremba.com/blog/use-snowflake-and-duckdb-with-iceberg', 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/aoBcrcIgg3KyuCfqyBjN4Qxvut-Bnq3KQE5wn2djMqc.jpg?auto=webp&s=04ff9d467d4467c264f80f998cc30330b7243a27', 'width': 691, 'height': 410}, 'resolutions': [{'url': 'https://external-preview.redd.it/aoBcrcIgg3KyuCfqyBjN4Qxvut-Bnq3KQE5wn2djMqc.jpg?width=108&crop=smart&auto=webp&s=e28d8400e2069b4e0082085d7d20524a072a0b2e', 'width': 108, 'height': 64}, {'url': 'https://external-preview.redd.it/aoBcrcIgg3KyuCfqyBjN4Qxvut-Bnq3KQE5wn2djMqc.jpg?width=216&crop=smart&auto=webp&s=a1441ff46b9026317bb5ea42a77fd34efdf80766', 'width': 216, 'height': 128}, {'url': 'https://external-preview.redd.it/aoBcrcIgg3KyuCfqyBjN4Qxvut-Bnq3KQE5wn2djMqc.jpg?width=320&crop=smart&auto=webp&s=851a0c8168fe5eb6257583d169cd8f89514bf1a5', 'width': 320, 'height': 189}, {'url': 'https://external-preview.redd.it/aoBcrcIgg3KyuCfqyBjN4Qxvut-Bnq3KQE5wn2djMqc.jpg?width=640&crop=smart&auto=webp&s=53f22684bbf6e8238df93b669ee92303cb2613a6', 'width': 640, 'height': 379}], 'variants': {}, 'id': 'O_FDlYO1bes_pjSw4qDF0Sl1ce9r4OiGmY7IqLuzbeM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e6r5no', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Buremba'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6r5no/query_snowflake_iceberg_tables_with_duckdb_spark/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://buremba.com/blog/use-snowflake-and-duckdb-with-iceberg', 'subreddit_subscribers': 198067, 'created_utc': 1721350535.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.597+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi to everyone,\n\nI'm pretty new to creating data infraestructure and i'm trying to design something like a data clean room for my company where we can share data with our partners while encrypting all personal identifiable information. The issue here is that i have several doubts on how to achieve this fairly for each part:  \n1. how can I ensure that the other's part data is trully encrypted? maybe asking them to encrypt it before sharing?  \n2. how can I prevent downloading but enable data manipulation with open source tecnologies? eg: *** for storage and python / R for data analysis.   \n3. where can I read more about this kind of architecture? some documented project or example i can read?\n\nThank's in advance.", 'author_fullname': 't2_ji9a5qf64', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Create something like my data clean room', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1e7c8uh', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 3, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 3, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721417456.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi to everyone,</p>\n\n<p>I&#39;m pretty new to creating data infraestructure and i&#39;m trying to design something like a data clean room for my company where we can share data with our partners while encrypting all personal identifiable information. The issue here is that i have several doubts on how to achieve this fairly for each part:<br/>\n1. how can I ensure that the other&#39;s part data is trully encrypted? maybe asking them to encrypt it before sharing?<br/>\n2. how can I prevent downloading but enable data manipulation with open source tecnologies? eg: *** for storage and python / R for data analysis.<br/>\n3. where can I read more about this kind of architecture? some documented project or example i can read?</p>\n\n<p>Thank&#39;s in advance.</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e7c8uh', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='juanmac93'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e7c8uh/create_something_like_my_data_clean_room/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e7c8uh/create_something_like_my_data_clean_room/', 'subreddit_subscribers': 198067, 'created_utc': 1721417456.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.597+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'For anyone preparing for the dbt Analytics Engineering Certification Exam, [dataflakes.io](http://dataflakes.io) provides free practice exams that closely simulate the real exam, helping you to identify knowledge gaps and build confidence. Feel free to check it out - 180 questions across the different categories paired with a kickass exam engine for free. ', 'author_fullname': 't2_jo19l4w1t', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'dbt Analytics Engineering Certification Exam', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e7669k', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.76, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721402100.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>For anyone preparing for the dbt Analytics Engineering Certification Exam, <a href="http://dataflakes.io">dataflakes.io</a> provides free practice exams that closely simulate the real exam, helping you to identify knowledge gaps and build confidence. Feel free to check it out - 180 questions across the different categories paired with a kickass exam engine for free. </p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1e7669k', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='dataflakes'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e7669k/dbt_analytics_engineering_certification_exam/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e7669k/dbt_analytics_engineering_certification_exam/', 'subreddit_subscribers': 198067, 'created_utc': 1721402100.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.597+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'I\'m building a new unstructured data engine, my cofounder and I call this "Snowflake" but for unstructured data. \n\n**This tool is still very early, so I like to hear feedback from data friends.** Basically, it\n\n- can be pointed to S3 location where unstructured files like docs, images, videos etc are lived\n\n- has an SQL interface to query files in S3 location (see below)\n\n- In the UI, you can configure LLM agents that take various input types. For example, if you were to classify some metadata like is\\_horror\\_movie from videos, you can roughly do something like this\n\n`SELECT EXTRACT(<agent_id>, video) FROM videos;`\n\n- In the UI, you can configure a semantic search index. Together with conventional SQL search, you can do pretty interesting hybrid SQL search query: \n\n`SELECT photo FROM photos WHERE photo LIKE \'golden gate bridge\' AND date = \'2023-11-13\';`\n\n**For those who start to think about using Gen AI to deal with unstructured data, how are you planning to do it?**', 'author_fullname': 't2_784lbg86', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data engineers who deal with unstructured data I need your help!', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6ukam', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.67, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721361353.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>I&#39;m building a new unstructured data engine, my cofounder and I call this &quot;Snowflake&quot; but for unstructured data. </p>\n\n<p><strong>This tool is still very early, so I like to hear feedback from data friends.</strong> Basically, it</p>\n\n<ul>\n<li><p>can be pointed to S3 location where unstructured files like docs, images, videos etc are lived</p></li>\n<li><p>has an SQL interface to query files in S3 location (see below)</p></li>\n<li><p>In the UI, you can configure LLM agents that take various input types. For example, if you were to classify some metadata like is_horror_movie from videos, you can roughly do something like this</p></li>\n</ul>\n\n<p><code>SELECT EXTRACT(&lt;agent_id&gt;, video) FROM videos;</code></p>\n\n<ul>\n<li>In the UI, you can configure a semantic search index. Together with conventional SQL search, you can do pretty interesting hybrid SQL search query: </li>\n</ul>\n\n<p><code>SELECT photo FROM photos WHERE photo LIKE &#39;golden gate bridge&#39; AND date = &#39;2023-11-13&#39;;</code></p>\n\n<p><strong>For those who start to think about using Gen AI to deal with unstructured data, how are you planning to do it?</strong></p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e6ukam', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='No_Communication2618'), 'discussion_type': None, 'num_comments': 6, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6ukam/data_engineers_who_deal_with_unstructured_data_i/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e6ukam/data_engineers_who_deal_with_unstructured_data_i/', 'subreddit_subscribers': 198067, 'created_utc': 1721361353.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.598+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Hi everyone,\n\nI'm a data engineer looking to upskill myself in real-time and batch data processing. I'm particularly interested in mastering Apache Flink, Spark, and Kafka. However, I don't have a strong background in Java, which I have heard is a popular, stable, and mature language for developing applications that interact with these tools.\n\nIs it worth investing time in learning Java to master these tools?\n\nI'd appreciate honest insights from experienced data engineers who have navigated this path or used these tools in production systems. Thanks!", 'author_fullname': 't2_4fkak3iu', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Learning Java for Data Engineering ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': True, 'name': 't3_1e7btod', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.75, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 2, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Discussion', 'can_mod_post': False, 'score': 2, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': 1721417429.0, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721416377.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi everyone,</p>\n\n<p>I&#39;m a data engineer looking to upskill myself in real-time and batch data processing. I&#39;m particularly interested in mastering Apache Flink, Spark, and Kafka. However, I don&#39;t have a strong background in Java, which I have heard is a popular, stable, and mature language for developing applications that interact with these tools.</p>\n\n<p>Is it worth investing time in learning Java to master these tools?</p>\n\n<p>I&#39;d appreciate honest insights from experienced data engineers who have navigated this path or used these tools in production systems. Thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': False, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '92b74b58-aaca-11eb-b160-0e6181e3773f', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ff4500', 'id': '1e7btod', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='nifesimii'), 'discussion_type': None, 'num_comments': 4, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e7btod/learning_java_for_data_engineering/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e7btod/learning_java_for_data_engineering/', 'subreddit_subscribers': 198067, 'created_utc': 1721416377.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.598+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': "Currently I'm pursuing my masters degree..! As I'm writing this post ,I need study partner to maintain discipline ,to stay motivated throughout my journey and to exchange idea to excel in Data engineering domain. so like minded peps feel free to dm me ..!", 'author_fullname': 't2_zutmts1my', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Looking for study partner ', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e79ger', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 1.0, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Career', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721410362.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Currently I&#39;m pursuing my masters degree..! As I&#39;m writing this post ,I need study partner to maintain discipline ,to stay motivated throughout my journey and to exchange idea to excel in Data engineering domain. so like minded peps feel free to dm me ..!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '069dd614-a7dc-11eb-8e48-0e90f49436a3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#349e48', 'id': '1e79ger', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='No_Initiative3642'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e79ger/looking_for_study_partner/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e79ger/looking_for_study_partner/', 'subreddit_subscribers': 198067, 'created_utc': 1721410362.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.599+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_h209j', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Data Contracts in Action: Testing', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 140, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6s33v', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.6, 'author_flair_background_color': None, 'ups': 1, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 1, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/ICMuNiLIEJW0CHCUJzlPzXKeBsNMst1SYVRTX9ZENkM.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1721353371.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'medium.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://medium.com/@pflooky/data-contracts-in-action-testing-111631338657?sk=8cea6c4e08a48d7e123b07b3e6a27713', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/NL1fL0P0HSxCdfVyZo6S2wuaAMjA8KbTHX1I_7BOI2E.jpg?auto=webp&s=938d71d759597c5dfc40556f3378733686bbee4f', 'width': 1200, 'height': 1379}, 'resolutions': [{'url': 'https://external-preview.redd.it/NL1fL0P0HSxCdfVyZo6S2wuaAMjA8KbTHX1I_7BOI2E.jpg?width=108&crop=smart&auto=webp&s=973dc9894a181ff0c4568a77d97ae0cc110d705c', 'width': 108, 'height': 124}, {'url': 'https://external-preview.redd.it/NL1fL0P0HSxCdfVyZo6S2wuaAMjA8KbTHX1I_7BOI2E.jpg?width=216&crop=smart&auto=webp&s=b27622f54633754bfae258bb9e64e468fd6a0d75', 'width': 216, 'height': 248}, {'url': 'https://external-preview.redd.it/NL1fL0P0HSxCdfVyZo6S2wuaAMjA8KbTHX1I_7BOI2E.jpg?width=320&crop=smart&auto=webp&s=123ff92917586585e4118da03d8c9797e9d307ab', 'width': 320, 'height': 367}, {'url': 'https://external-preview.redd.it/NL1fL0P0HSxCdfVyZo6S2wuaAMjA8KbTHX1I_7BOI2E.jpg?width=640&crop=smart&auto=webp&s=1d25de5887d1b087cd123a547690fbd5e65c7bc3', 'width': 640, 'height': 735}, {'url': 'https://external-preview.redd.it/NL1fL0P0HSxCdfVyZo6S2wuaAMjA8KbTHX1I_7BOI2E.jpg?width=960&crop=smart&auto=webp&s=99cd6e9591820c0b1fd2dc16eb937c56a3cafbf5', 'width': 960, 'height': 1103}, {'url': 'https://external-preview.redd.it/NL1fL0P0HSxCdfVyZo6S2wuaAMjA8KbTHX1I_7BOI2E.jpg?width=1080&crop=smart&auto=webp&s=0edc9dc0bc499ea905b5829f2f802067a201b9d9', 'width': 1080, 'height': 1241}], 'variants': {}, 'id': '9CghMxfqx6aD-R-p0fUuORNzJ3cF5aun3CMaXVr-lnM'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e6s33v', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='Pitah7'), 'discussion_type': None, 'num_comments': 0, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6s33v/data_contracts_in_action_testing/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://medium.com/@pflooky/data-contracts-in-action-testing-111631338657?sk=8cea6c4e08a48d7e123b07b3e6a27713', 'subreddit_subscribers': 198067, 'created_utc': 1721353371.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.599+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': 'Hi\n\n  \nCan someone outline the tech stack needed to carry out the following steps:\n\n  \n1. Call an API\n\n2. Decode the JSON file into a flat file\n\n3. Store that file in a table in a SQL warehouse\n\n  \nI want to use an Azure warehouse. Should I use ADF and then a python script to decode the JSON file? Do I need some sort of blob storage as a holding or can I just process the json file in memory? The data payload is not very large. \n\n  \nThanks!\n\n  \n', 'author_fullname': 't2_xfqc5', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'Call and API, decode and the store in a SQL warehouse', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': None, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e78t8l', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.5, 'author_flair_background_color': None, 'subreddit_type': 'public', 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': None, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Help', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'self', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'content_categories': None, 'is_self': True, 'mod_note': None, 'created': 1721408736.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'self.dataengineering', 'allow_live_comments': False, 'selftext_html': '<!-- SC_OFF --><div class="md"><p>Hi</p>\n\n<p>Can someone outline the tech stack needed to carry out the following steps:</p>\n\n<ol>\n<li><p>Call an API</p></li>\n<li><p>Decode the JSON file into a flat file</p></li>\n<li><p>Store that file in a table in a SQL warehouse</p></li>\n</ol>\n\n<p>I want to use an Azure warehouse. Should I use ADF and then a python script to decode the JSON file? Do I need some sort of blob storage as a holding or can I just process the json file in memory? The data payload is not very large. </p>\n\n<p>Thanks!</p>\n</div><!-- SC_ON -->', 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': '2ca94cd6-ac27-11eb-a8eb-0e7f457f5bd3', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'num_reports': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'removal_reason': None, 'link_flair_background_color': '#ea0027', 'id': '1e78t8l', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='random_postings'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e78t8l/call_and_api_decode_and_the_store_in_a_sql/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.reddit.com/r/dataengineering/comments/1e78t8l/call_and_api_decode_and_the_store_in_a_sql/', 'subreddit_subscribers': 198067, 'created_utc': 1721408736.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.599+0000] {logging_mixin.py:188} INFO - {'comment_limit': 2048, 'comment_sort': 'confidence', '_reddit': <praw.reddit.Reddit object at 0x7f0ed815cdc0>, 'approved_at_utc': None, 'subreddit': Subreddit(display_name='dataengineering'), 'selftext': '', 'author_fullname': 't2_8d5mczd0', 'saved': False, 'mod_reason_title': None, 'gilded': 0, 'clicked': False, 'title': 'No Bullshit BI', 'link_flair_richtext': [], 'subreddit_name_prefixed': 'r/dataengineering', 'hidden': False, 'pwls': 6, 'link_flair_css_class': '', 'downs': 0, 'thumbnail_height': 70, 'top_awarded_type': None, 'hide_score': False, 'name': 't3_1e6vdob', 'quarantine': False, 'link_flair_text_color': 'light', 'upvote_ratio': 0.38, 'author_flair_background_color': None, 'ups': 0, 'total_awards_received': 0, 'media_embed': {}, 'thumbnail_width': 140, 'author_flair_template_id': None, 'is_original_content': False, 'user_reports': [], 'secure_media': None, 'is_reddit_media_domain': False, 'is_meta': False, 'category': None, 'secure_media_embed': {}, 'link_flair_text': 'Blog', 'can_mod_post': False, 'score': 0, 'approved_by': None, 'is_created_from_ads_ui': False, 'author_premium': False, 'thumbnail': 'https://b.thumbs.redditmedia.com/uO2H8B7B48ExR7Vb13F2ebkM4awejE0nrGucFYIUq4o.jpg', 'edited': False, 'author_flair_css_class': None, 'author_flair_richtext': [], 'gildings': {}, 'post_hint': 'link', 'content_categories': None, 'is_self': False, 'subreddit_type': 'public', 'created': 1721364171.0, 'link_flair_type': 'text', 'wls': 6, 'removed_by_category': None, 'banned_by': None, 'author_flair_type': 'text', 'domain': 'thdpth.com', 'allow_live_comments': False, 'selftext_html': None, 'likes': None, 'suggested_sort': None, 'banned_at_utc': None, 'url_overridden_by_dest': 'https://www.thdpth.com/p/no-bullshit-bi', 'view_count': None, 'archived': False, 'no_follow': True, 'is_crosspostable': False, 'pinned': False, 'over_18': False, 'preview': {'images': [{'source': {'url': 'https://external-preview.redd.it/lPbAdCW1_bWvXCdc7s_wPejVLHkS2DL9go7vj8PoY_w.jpg?auto=webp&s=a414ff3d8281f17439236aba3f7dc60152bef0a1', 'width': 1200, 'height': 600}, 'resolutions': [{'url': 'https://external-preview.redd.it/lPbAdCW1_bWvXCdc7s_wPejVLHkS2DL9go7vj8PoY_w.jpg?width=108&crop=smart&auto=webp&s=3ae61bdadc2d48bb04b890718f3cbfc199996aee', 'width': 108, 'height': 54}, {'url': 'https://external-preview.redd.it/lPbAdCW1_bWvXCdc7s_wPejVLHkS2DL9go7vj8PoY_w.jpg?width=216&crop=smart&auto=webp&s=6f5dd22f6cc759c0ef6f6bea0c2a620a94910cd3', 'width': 216, 'height': 108}, {'url': 'https://external-preview.redd.it/lPbAdCW1_bWvXCdc7s_wPejVLHkS2DL9go7vj8PoY_w.jpg?width=320&crop=smart&auto=webp&s=3ad8d3050819e4242f9d05cf479ddbffc59d383c', 'width': 320, 'height': 160}, {'url': 'https://external-preview.redd.it/lPbAdCW1_bWvXCdc7s_wPejVLHkS2DL9go7vj8PoY_w.jpg?width=640&crop=smart&auto=webp&s=24567ea9ba2fbfcbb72fc8a7645fb4e2e69f922f', 'width': 640, 'height': 320}, {'url': 'https://external-preview.redd.it/lPbAdCW1_bWvXCdc7s_wPejVLHkS2DL9go7vj8PoY_w.jpg?width=960&crop=smart&auto=webp&s=f44bb0d0f7b1de18e9bb049f8ee3f9e931939a77', 'width': 960, 'height': 480}, {'url': 'https://external-preview.redd.it/lPbAdCW1_bWvXCdc7s_wPejVLHkS2DL9go7vj8PoY_w.jpg?width=1080&crop=smart&auto=webp&s=de73122fb44ab3573aa8150f9f35c7e4550c20cb', 'width': 1080, 'height': 540}], 'variants': {}, 'id': 'e0x7mdASK_QK5ZYJgxseAdTzsG09oFdXy-ahaa63m4o'}], 'enabled': False}, 'all_awardings': [], 'awarders': [], 'media_only': False, 'link_flair_template_id': 'eb739554-a7db-11eb-95d7-0ec0f8f30313', 'can_gild': False, 'spoiler': False, 'locked': False, 'author_flair_text': None, 'treatment_tags': [], 'visited': False, 'removed_by': None, 'mod_note': None, 'distinguished': None, 'subreddit_id': 't5_36en4', 'author_is_blocked': False, 'mod_reason_by': None, 'num_reports': None, 'removal_reason': None, 'link_flair_background_color': '#0079d3', 'id': '1e6vdob', 'is_robot_indexable': True, 'report_reasons': None, 'author': Redditor(name='sbalnojan'), 'discussion_type': None, 'num_comments': 1, 'send_replies': True, 'whitelist_status': 'all_ads', 'contest_mode': False, 'mod_reports': [], 'author_patreon_flair': False, 'author_flair_text_color': None, 'permalink': '/r/dataengineering/comments/1e6vdob/no_bullshit_bi/', 'parent_whitelist_status': 'all_ads', 'stickied': False, 'url': 'https://www.thdpth.com/p/no-bullshit-bi', 'subreddit_subscribers': 198067, 'created_utc': 1721364171.0, 'num_crossposts': 0, 'media': None, 'is_video': False, '_fetched': False, '_additional_fetch_params': {}, '_comments_by_id': {}}
[2024-07-19T20:58:27.600+0000] {python.py:237} INFO - Done. Returned value was: None
[2024-07-19T20:58:27.601+0000] {taskinstance.py:441} INFO - ::group::Post task execution logs
[2024-07-19T20:58:27.608+0000] {taskinstance.py:1206} INFO - Marking task as SUCCESS. dag_id=etl_reddit_pipeline, task_id=reddit_extraction, run_id=manual__2024-07-19T20:58:25.504369+00:00, execution_date=20240719T205825, start_date=20240719T205826, end_date=20240719T205827
[2024-07-19T20:58:27.666+0000] {local_task_job_runner.py:243} INFO - Task exited with return code 0
[2024-07-19T20:58:27.682+0000] {taskinstance.py:3503} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2024-07-19T20:58:27.683+0000] {local_task_job_runner.py:222} INFO - ::endgroup::
